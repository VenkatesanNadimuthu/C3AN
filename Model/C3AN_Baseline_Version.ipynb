{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOcIgFfeylAg2O7tZ2BmmLt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkatesanNadimuthu/C3AN/blob/main/Model/C3AN_Baseline_Version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TrsQ15tjQK3k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9af6de6b-512d-4c5a-d4b2-5e677bc0020c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training new BPE tokenizer on /content/south_indian_recipes_fixed.txt...\n",
            "Tokenizer trained and saved.\n",
            "Vocabulary Size: 1414\n",
            "Detected sequence length from first row (with BOS/EOS): 111\n",
            "Model initialized.\n",
            "Starting training...\n",
            "Epoch 1 | Step 0 | Loss: 7.3125 | LR: 0.000004\n",
            "Epoch 1 | Step 50 | Loss: 6.8465 | LR: 0.000005\n",
            "Epoch 1 | Step 100 | Loss: 6.5933 | LR: 0.000007\n",
            "Epoch 1 | Step 150 | Loss: 6.3646 | LR: 0.000010\n",
            "Epoch 1 | Step 200 | Loss: 6.0626 | LR: 0.000014\n",
            "Epoch 1 | Step 250 | Loss: 5.5563 | LR: 0.000020\n",
            "Epoch 1 | Step 300 | Loss: 5.0060 | LR: 0.000026\n",
            "==> End of Epoch 1, Average Loss: 6.2076\n",
            "Epoch 2 | Step 0 | Loss: 4.8523 | LR: 0.000028\n",
            "Epoch 2 | Step 50 | Loss: 4.2406 | LR: 0.000035\n",
            "Epoch 2 | Step 100 | Loss: 3.6366 | LR: 0.000043\n",
            "Epoch 2 | Step 150 | Loss: 2.9527 | LR: 0.000051\n",
            "Epoch 2 | Step 200 | Loss: 2.3100 | LR: 0.000059\n",
            "Epoch 2 | Step 250 | Loss: 1.8015 | LR: 0.000067\n",
            "Epoch 2 | Step 300 | Loss: 1.3146 | LR: 0.000074\n",
            "==> End of Epoch 2, Average Loss: 2.9201\n",
            "Epoch 3 | Step 0 | Loss: 1.1759 | LR: 0.000076\n",
            "Epoch 3 | Step 50 | Loss: 0.8672 | LR: 0.000083\n",
            "Epoch 3 | Step 100 | Loss: 0.6338 | LR: 0.000089\n",
            "Epoch 3 | Step 150 | Loss: 0.4987 | LR: 0.000093\n",
            "Epoch 3 | Step 200 | Loss: 0.4174 | LR: 0.000097\n",
            "Epoch 3 | Step 250 | Loss: 0.3536 | LR: 0.000099\n",
            "Epoch 3 | Step 300 | Loss: 0.3309 | LR: 0.000100\n",
            "==> End of Epoch 3, Average Loss: 0.5735\n",
            "Epoch 4 | Step 0 | Loss: 0.3187 | LR: 0.000100\n",
            "Epoch 4 | Step 50 | Loss: 0.3015 | LR: 0.000100\n",
            "Epoch 4 | Step 100 | Loss: 0.2817 | LR: 0.000099\n",
            "Epoch 4 | Step 150 | Loss: 0.2643 | LR: 0.000099\n",
            "Epoch 4 | Step 200 | Loss: 0.2510 | LR: 0.000098\n",
            "Epoch 4 | Step 250 | Loss: 0.2349 | LR: 0.000097\n",
            "Epoch 4 | Step 300 | Loss: 0.2211 | LR: 0.000095\n",
            "==> End of Epoch 4, Average Loss: 0.2633\n",
            "Epoch 5 | Step 0 | Loss: 0.2321 | LR: 0.000095\n",
            "Epoch 5 | Step 50 | Loss: 0.2251 | LR: 0.000093\n",
            "Epoch 5 | Step 100 | Loss: 0.2165 | LR: 0.000091\n",
            "Epoch 5 | Step 150 | Loss: 0.2155 | LR: 0.000089\n",
            "Epoch 5 | Step 200 | Loss: 0.1970 | LR: 0.000087\n",
            "Epoch 5 | Step 250 | Loss: 0.2038 | LR: 0.000084\n",
            "Epoch 5 | Step 300 | Loss: 0.2075 | LR: 0.000082\n",
            "==> End of Epoch 5, Average Loss: 0.2115\n",
            "Epoch 6 | Step 0 | Loss: 0.1965 | LR: 0.000081\n",
            "Epoch 6 | Step 50 | Loss: 0.1993 | LR: 0.000078\n",
            "Epoch 6 | Step 100 | Loss: 0.1920 | LR: 0.000075\n",
            "Epoch 6 | Step 150 | Loss: 0.1824 | LR: 0.000072\n",
            "Epoch 6 | Step 200 | Loss: 0.1866 | LR: 0.000069\n",
            "Epoch 6 | Step 250 | Loss: 0.1907 | LR: 0.000065\n",
            "Epoch 6 | Step 300 | Loss: 0.1892 | LR: 0.000062\n",
            "==> End of Epoch 6, Average Loss: 0.1924\n",
            "Epoch 7 | Step 0 | Loss: 0.1867 | LR: 0.000061\n",
            "Epoch 7 | Step 50 | Loss: 0.1778 | LR: 0.000057\n",
            "Epoch 7 | Step 100 | Loss: 0.1800 | LR: 0.000054\n",
            "Epoch 7 | Step 150 | Loss: 0.1758 | LR: 0.000050\n",
            "Epoch 7 | Step 200 | Loss: 0.1698 | LR: 0.000047\n",
            "Epoch 7 | Step 250 | Loss: 0.1735 | LR: 0.000043\n",
            "Epoch 7 | Step 300 | Loss: 0.1743 | LR: 0.000040\n",
            "==> End of Epoch 7, Average Loss: 0.1765\n",
            "Epoch 8 | Step 0 | Loss: 0.1711 | LR: 0.000039\n",
            "Epoch 8 | Step 50 | Loss: 0.1680 | LR: 0.000035\n",
            "Epoch 8 | Step 100 | Loss: 0.1673 | LR: 0.000032\n",
            "Epoch 8 | Step 150 | Loss: 0.1638 | LR: 0.000029\n",
            "Epoch 8 | Step 200 | Loss: 0.1660 | LR: 0.000025\n",
            "Epoch 8 | Step 250 | Loss: 0.1616 | LR: 0.000022\n",
            "Epoch 8 | Step 300 | Loss: 0.1599 | LR: 0.000019\n",
            "==> End of Epoch 8, Average Loss: 0.1654\n",
            "Epoch 9 | Step 0 | Loss: 0.1612 | LR: 0.000019\n",
            "Epoch 9 | Step 50 | Loss: 0.1603 | LR: 0.000016\n",
            "Epoch 9 | Step 100 | Loss: 0.1594 | LR: 0.000013\n",
            "Epoch 9 | Step 150 | Loss: 0.1566 | LR: 0.000011\n",
            "Epoch 9 | Step 200 | Loss: 0.1563 | LR: 0.000009\n",
            "Epoch 9 | Step 250 | Loss: 0.1596 | LR: 0.000007\n",
            "Epoch 9 | Step 300 | Loss: 0.1588 | LR: 0.000005\n",
            "==> End of Epoch 9, Average Loss: 0.1605\n",
            "Epoch 10 | Step 0 | Loss: 0.1585 | LR: 0.000005\n",
            "Epoch 10 | Step 50 | Loss: 0.1604 | LR: 0.000003\n",
            "Epoch 10 | Step 100 | Loss: 0.1563 | LR: 0.000002\n",
            "Epoch 10 | Step 150 | Loss: 0.1619 | LR: 0.000001\n",
            "Epoch 10 | Step 200 | Loss: 0.1630 | LR: 0.000001\n",
            "Epoch 10 | Step 250 | Loss: 0.1582 | LR: 0.000000\n",
            "Epoch 10 | Step 300 | Loss: 0.1582 | LR: 0.000000\n",
            "==> End of Epoch 10, Average Loss: 0.1591\n",
            "Model saved to decoder_model.pth\n"
          ]
        }
      ],
      "source": [
        "#Step:1 Model Training\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. Hyperparameters & Configuration\n",
        "# ==========================================\n",
        "class Config:\n",
        "    # Model Architecture\n",
        "    d_model = 256           # Embedding dimension\n",
        "    n_head = 8              # Number of attention heads (changed from 6 to 8)\n",
        "    n_layer = 6             # Number of decoder layers\n",
        "    vocab_size = 5000       # Size of BPE vocabulary\n",
        "    dropout = 0.1\n",
        "\n",
        "    # Training\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.0001\n",
        "    epochs = 10\n",
        "\n",
        "    # File Paths\n",
        "    # TODO: UPDATE THIS PATH to your actual dataset file (.csv or .txt)\n",
        "    dataset_path = '/content/south_indian_recipes_fixed.txt'\n",
        "\n",
        "    # TODO: IF USING CSV, UPDATE THIS to the specific column header containing the text\n",
        "    csv_column = 'text_column_name'\n",
        "\n",
        "    tokenizer_path = 'bpe_tokenizer.json'\n",
        "    model_save_path = 'decoder_model.pth'\n",
        "\n",
        "    # Hardware\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==========================================\n",
        "# 2. Tokenizer Setup (BPE)\n",
        "# ==========================================\n",
        "def get_training_corpus(dataset_path, column_name):\n",
        "    \"\"\"Yields text from the file (CSV or TXT) line by line.\"\"\"\n",
        "    is_csv = dataset_path.lower().endswith('.csv')\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        if is_csv:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                if row[column_name]:\n",
        "                    yield row[column_name]\n",
        "        else:\n",
        "            # Assume plain text file, one sequence per line\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    yield line.strip()\n",
        "\n",
        "def train_or_load_tokenizer(config):\n",
        "    \"\"\"\n",
        "    Trains a BPE tokenizer on the dataset if it doesn't exist,\n",
        "    otherwise loads the existing one.\n",
        "    \"\"\"\n",
        "    if os.path.exists(config.tokenizer_path):\n",
        "        print(f\"Loading existing tokenizer from {config.tokenizer_path}...\")\n",
        "        tokenizer = Tokenizer.from_file(config.tokenizer_path)\n",
        "    else:\n",
        "        print(f\"Training new BPE tokenizer on {config.dataset_path}...\")\n",
        "        # Initialize BPE tokenizer\n",
        "        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "        # Trainer spec\n",
        "        trainer = trainers.BpeTrainer(\n",
        "            vocab_size=config.vocab_size,\n",
        "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "        )\n",
        "\n",
        "        # Train on the CSV iterator\n",
        "        tokenizer.train_from_iterator(\n",
        "            get_training_corpus(config.dataset_path, config.csv_column),\n",
        "            trainer=trainer\n",
        "        )\n",
        "\n",
        "        # Post-processing\n",
        "        tokenizer.save(config.tokenizer_path)\n",
        "        print(\"Tokenizer trained and saved.\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "# ==========================================\n",
        "# 3. Dataset Class\n",
        "# ==========================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size, column_name):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load data based on file type\n",
        "        self.data = []\n",
        "        is_csv = file_path.lower().endswith('.csv')\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            if is_csv:\n",
        "                reader = csv.DictReader(f)\n",
        "                self.data = [row[column_name] for row in reader if row[column_name]]\n",
        "            else:\n",
        "                self.data = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # We will handle padding manually in __getitem__ to ensure [BOS] and [EOS] placement\n",
        "        # self.tokenizer.enable_padding(length=block_size, pad_id=0, pad_token=\"[PAD]\")\n",
        "        # self.tokenizer.enable_truncation(max_length=block_size)\n",
        "\n",
        "        # Cache special token IDs\n",
        "        self.bos_id = self.tokenizer.token_to_id(\"[BOS]\")\n",
        "        self.eos_id = self.tokenizer.token_to_id(\"[EOS]\")\n",
        "        self.pad_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "\n",
        "        # Encode (without padding/truncation initially)\n",
        "        encoded = self.tokenizer.encode(text)\n",
        "        ids = encoded.ids\n",
        "\n",
        "        # Add BOS and EOS\n",
        "        # Ensure we have valid IDs for BOS/EOS\n",
        "        bos = [self.bos_id] if self.bos_id is not None else []\n",
        "        eos = [self.eos_id] if self.eos_id is not None else []\n",
        "\n",
        "        ids = bos + ids + eos\n",
        "\n",
        "        # Truncate if necessary (keep room for BOS/EOS)\n",
        "        if len(ids) > self.block_size:\n",
        "            ids = ids[:self.block_size]\n",
        "            # Ensure EOS is still at the end if we truncated it (optional, but good practice)\n",
        "            if self.eos_id is not None:\n",
        "                ids[-1] = self.eos_id\n",
        "\n",
        "        # Pad\n",
        "        padding_len = self.block_size - len(ids)\n",
        "        if padding_len > 0:\n",
        "            ids = ids + [self.pad_id] * padding_len\n",
        "\n",
        "        ids_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "        return ids_tensor\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model Architecture (Decoder Only)\n",
        "# ==========================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
        "        self.n_head = config.n_head\n",
        "        self.d_model = config.d_model\n",
        "\n",
        "        # Register buffer for the causal mask (lower triangular matrix)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048))\n",
        "                                     .view(1, 1, 2048, 2048))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch, Time(seq_len), Channels(dim)\n",
        "\n",
        "        # Calculate query, key, values\n",
        "        q, k, v  = self.c_attn(x).split(self.d_model, dim=2)\n",
        "\n",
        "        # Transpose for multi-head attention: (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # Causal Attention (Masked)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "\n",
        "        # Re-assemble all head outputs side by side\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.sa = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm architecture\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(2048, config.d_model) # Max pos 2048\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Create position indices [0, 1, ..., T-1]\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "\n",
        "        # Forward pass\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Flatten for CrossEntropyLoss\n",
        "            # Logits: (B*T, VocabSize)\n",
        "            # Targets: (B*T)\n",
        "            # FIX: Ignore padding index (0)\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=0)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Routine\n",
        "# ==========================================\n",
        "def main():\n",
        "    # A. Setup\n",
        "    config = Config()\n",
        "    print(f\"Using device: {config.device}\")\n",
        "\n",
        "    # Check for data\n",
        "    if not os.path.exists(config.dataset_path):\n",
        "        print(f\"Error: Dataset file '{config.dataset_path}' not found.\")\n",
        "        print(\"Please update Config.dataset_path in the script to point to your actual data file.\")\n",
        "        return\n",
        "\n",
        "    # B. Tokenizer\n",
        "    tokenizer = train_or_load_tokenizer(config)\n",
        "    config.vocab_size = tokenizer.get_vocab_size()\n",
        "    print(f\"Vocabulary Size: {config.vocab_size}\")\n",
        "\n",
        "    # C. Prepare Data\n",
        "    # Determine block size from first line of CSV or TXT\n",
        "    is_csv = config.dataset_path.lower().endswith('.csv')\n",
        "    first_text = \"\"\n",
        "\n",
        "    with open(config.dataset_path, 'r', encoding='utf-8') as f:\n",
        "        if is_csv:\n",
        "            reader = csv.DictReader(f)\n",
        "            try:\n",
        "                first_row = next(reader)\n",
        "                if config.csv_column not in first_row:\n",
        "                    print(f\"Error: Column '{config.csv_column}' not found in CSV.\")\n",
        "                    print(f\"Found headers: {list(first_row.keys())}\")\n",
        "                    print(\"Please update Config.csv_column to match one of these headers.\")\n",
        "                    return\n",
        "                first_text = first_row[config.csv_column]\n",
        "            except StopIteration:\n",
        "                print(f\"Error: The CSV file '{config.dataset_path}' appears to be empty.\")\n",
        "                return\n",
        "        else:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    first_text = line.strip()\n",
        "                    break\n",
        "            if not first_text:\n",
        "                print(\"Error: Dataset file appears empty.\")\n",
        "                return\n",
        "\n",
        "    # Simple heuristic for max len (add buffer for BOS/EOS)\n",
        "    dummy_enc = tokenizer.encode(first_text)\n",
        "    detected_len = len(dummy_enc.ids) + 2\n",
        "    print(f\"Detected sequence length from first row (with BOS/EOS): {detected_len}\")\n",
        "\n",
        "    block_size = detected_len\n",
        "    dataset = TextDataset(config.dataset_path, tokenizer, block_size, config.csv_column)\n",
        "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
        "\n",
        "    # D. Model Initialization\n",
        "    model = TransformerDecoder(config).to(config.device)\n",
        "    print(\"Model initialized.\")\n",
        "\n",
        "    # E. Optimizer & Scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    # Scheduler: OneCycleLR\n",
        "    total_steps = len(dataloader) * config.epochs\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=config.learning_rate,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=0.3\n",
        "    )\n",
        "\n",
        "    # F. Training Loop\n",
        "    model.train()\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(dataloader):\n",
        "            # Move to device\n",
        "            batch = batch.to(config.device)\n",
        "\n",
        "            # Autoregressive setup:\n",
        "            # Input: [BOS, A, B, C, EOS]\n",
        "            # Target: [A, B, C, EOS, PAD]\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            # Forward\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(inputs, targets=targets)\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1} | Step {step} | Loss: {loss.item():.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"==> End of Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # G. Save\n",
        "    torch.save(model.state_dict(), config.model_save_path)\n",
        "    print(f\"Model saved to {config.model_save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "829aefff",
        "outputId": "79952e79-5365-401d-8b1e-c6d3a6c16754"
      },
      "source": [
        "# Step:2 Model Inferencing\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration and Setup\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tokenizers import Tokenizer\n",
        "import os\n",
        "\n",
        "# The Config class and model architecture (CausalSelfAttention, FeedForward, Block, TransformerDecoder)\n",
        "# are assumed to be defined and available from the execution of the previous training cell (TrsQ15tjQK3k).\n",
        "# We will use the existing Config object.\n",
        "\n",
        "# Create an instance of the Config class to access paths and device settings\n",
        "config = Config()\n",
        "\n",
        "# ==========================================\n",
        "# 2. Load Tokenizer\n",
        "# ==========================================\n",
        "print(f\"Loading tokenizer from {config.tokenizer_path}...\")\n",
        "if not os.path.exists(config.tokenizer_path):\n",
        "    print(f\"Error: Tokenizer file '{config.tokenizer_path}' not found.\")\n",
        "    print(\"Please ensure the tokenizer is trained and saved in the previous step.\")\n",
        "    exit()\n",
        "tokenizer = Tokenizer.from_file(config.tokenizer_path)\n",
        "config.vocab_size = tokenizer.get_vocab_size() # Update vocab_size from loaded tokenizer\n",
        "print(f\"Tokenizer loaded. Vocabulary Size: {config.vocab_size}\")\n",
        "\n",
        "# Cache special token IDs for easier use\n",
        "bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
        "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. Load Model\n",
        "# ==========================================\n",
        "print(f\"Loading model from {config.model_save_path}...\")\n",
        "if not os.path.exists(config.model_save_path):\n",
        "    print(f\"Error: Model file '{config.model_save_path}' not found.\")\n",
        "    print(\"Please ensure the model is trained and saved in the previous step.\")\n",
        "    exit()\n",
        "\n",
        "# Instantiate the model (assuming TransformerDecoder class is defined globally)\n",
        "model = TransformerDecoder(config).to(config.device)\n",
        "model.load_state_dict(torch.load(config.model_save_path, map_location=config.device))\n",
        "model.eval() # Set model to evaluation mode for inference\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Text Generation Function\n",
        "# ==========================================\n",
        "def generate_text(model, tokenizer, prompt_text, max_length=200, temperature=0.7, top_k=40, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generates text from a given prompt using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained TransformerDecoder model.\n",
        "        tokenizer: The BPE tokenizer.\n",
        "        prompt_text (str): The initial text prompt.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Controls randomness in sampling. Lower values make output more deterministic.\n",
        "        top_k (int): If not None, sample from the top_k most probable tokens.\n",
        "        device (str): The device to run inference on ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    # Encode the prompt text\n",
        "    encoded_prompt = tokenizer.encode(prompt_text)\n",
        "    input_ids = encoded_prompt.ids\n",
        "\n",
        "    # Prepend BOS token if it's available and not already at the start\n",
        "    if bos_id is not None and (not input_ids or input_ids[0] != bos_id):\n",
        "        input_ids = [bos_id] + input_ids\n",
        "\n",
        "    # Convert to tensor and add batch dimension (batch_size=1)\n",
        "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # List to store all generated token IDs, starting with the prompt's IDs\n",
        "    generated_ids = list(input_ids)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations for inference\n",
        "        for _ in range(max_length - len(input_ids)): # Generate tokens up to max_length\n",
        "            # Get predictions (logits) from the model\n",
        "            logits, _ = model(input_ids_tensor)\n",
        "            # Focus on the logits for the last predicted token\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k sampling to logits\n",
        "            if top_k is not None:\n",
        "                # Get top_k values and their indices\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                # Set logits of tokens outside the top_k to a very low value (effectively zero probability)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Convert logits to probabilities and sample the next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "\n",
        "            # Append the newly generated token to the input tensor for the next step\n",
        "            input_ids_tensor = torch.cat((input_ids_tensor, next_token_id.unsqueeze(0)), dim=1)\n",
        "            generated_ids.append(next_token_id.item()) # Add to our tracking list\n",
        "\n",
        "            # Stop generation if EOS token is produced\n",
        "            if next_token_id.item() == eos_id:\n",
        "                break\n",
        "\n",
        "    # Decode the complete sequence of token IDs back into text\n",
        "    # Remove the BOS token if it was prepended by this function and is not part of desired output\n",
        "    if bos_id is not None and generated_ids and generated_ids[0] == bos_id:\n",
        "        generated_ids_for_decode = generated_ids[1:]\n",
        "    else:\n",
        "        generated_ids_for_decode = generated_ids\n",
        "\n",
        "    return tokenizer.decode(generated_ids_for_decode, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. Interactive User Input and Generation Loop\n",
        "# ==========================================\n",
        "print(\"\\n--- Recipe Generation Mode ---\")\n",
        "print(\"Enter a prompt to start generating a recipe. Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Your prompt: \")\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    if not user_prompt.strip():\n",
        "        print(\"Please enter a non-empty prompt.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating recipe based on your prompt...\")\n",
        "    generated_recipe = generate_text(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt_text=user_prompt,\n",
        "        max_length=200, # Max tokens for the generated output\n",
        "        temperature=0.8, # Adjust for creativity (higher = more creative, lower = more focused)\n",
        "        top_k=50,        # Adjust to control diversity (higher = more diverse token choices)\n",
        "        device=config.device\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generated Recipe ---\")\n",
        "    print(generated_recipe)\n",
        "    print(\"------------------------\\n\")\n",
        "\n",
        "print(\"Exiting recipe generation. Goodbye!\")\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from bpe_tokenizer.json...\n",
            "Tokenizer loaded. Vocabulary Size: 1414\n",
            "Loading model from decoder_model.pth...\n",
            "Model loaded successfully.\n",
            "\n",
            "--- Recipe Generation Mode ---\n",
            "Enter a prompt to start generating a recipe. Type 'exit' to quit.\n",
            "\n",
            "Your prompt: Let's make something traditional yet healthy today: Brown Rice Poriyal (Stir Fry) enriched with Drumstick. This recipe has been in my family for generations. The star ingredient here is Brown Rice\n",
            "\n",
            "Generating recipe based on your prompt...\n",
            "\n",
            "--- Generated Recipe ---\n",
            "Let ' s make something traditional yet healthy today : Brown Rice Poriyal ( Stir Fry ) enriched with Drumstick . This recipe has been in my family for generations . The star ingredient here is Brown Rice , which is packed with fiber . To start , wash the Brown Rice and Urad Dal thoroughly under running water until the water runs clear . Drain well . Now , heat your favorite clay pot or wok and add Ghee . When hot , splutter the Green Chilies . Immediately add the Drumstick , chopped into small uniform pieces to ensure even cooking . SautÃ© for 5 - 7\n",
            "------------------------\n",
            "\n",
            "Your prompt: Exit\n",
            "Exiting recipe generation. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HucaJmFVQY4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}