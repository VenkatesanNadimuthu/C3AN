{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VenkatesanNadimuthu/C3AN/blob/main/Model/C3AN_Baseline_Version_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrsQ15tjQK3k",
        "outputId": "0e6d284d-99e5-44c9-a600-5853d95bd336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Training new BPE tokenizer on /content/processed_recipes.txt...\n",
            "Tokenizer trained and saved.\n",
            "Vocabulary Size: 5000\n",
            "Detected sequence length from first row (with BOS/EOS): 268\n",
            "Dataset split: Training samples = 6941, Testing samples = 1736\n",
            "Model initialized.\n",
            "Starting training...\n",
            "Epoch 1 | Step 0 | Loss: 8.5948 | LR: 0.000004\n",
            "Epoch 1 | Step 50 | Loss: 7.5535 | LR: 0.000005\n",
            "Epoch 1 | Step 100 | Loss: 7.1878 | LR: 0.000010\n",
            "Epoch 1 | Step 150 | Loss: 6.7675 | LR: 0.000016\n",
            "Epoch 1 | Step 200 | Loss: 6.0353 | LR: 0.000025\n",
            "==> End of Epoch 1, Average Loss: 7.0794\n",
            "Epoch 2 | Step 0 | Loss: 5.8396 | LR: 0.000028\n",
            "Epoch 2 | Step 50 | Loss: 5.0606 | LR: 0.000039\n",
            "Epoch 2 | Step 100 | Loss: 4.3693 | LR: 0.000050\n",
            "Epoch 2 | Step 150 | Loss: 4.1471 | LR: 0.000062\n",
            "Epoch 2 | Step 200 | Loss: 3.8337 | LR: 0.000073\n",
            "==> End of Epoch 2, Average Loss: 4.5349\n",
            "Epoch 3 | Step 0 | Loss: 3.5210 | LR: 0.000076\n",
            "Epoch 3 | Step 50 | Loss: 3.6357 | LR: 0.000086\n",
            "Epoch 3 | Step 100 | Loss: 3.3859 | LR: 0.000093\n",
            "Epoch 3 | Step 150 | Loss: 3.1793 | LR: 0.000098\n",
            "Epoch 3 | Step 200 | Loss: 3.2317 | LR: 0.000100\n",
            "==> End of Epoch 3, Average Loss: 3.3343\n",
            "Epoch 4 | Step 0 | Loss: 2.9332 | LR: 0.000100\n",
            "Epoch 4 | Step 50 | Loss: 3.0609 | LR: 0.000100\n",
            "Epoch 4 | Step 100 | Loss: 3.0067 | LR: 0.000099\n",
            "Epoch 4 | Step 150 | Loss: 2.8321 | LR: 0.000098\n",
            "Epoch 4 | Step 200 | Loss: 2.8161 | LR: 0.000096\n",
            "==> End of Epoch 4, Average Loss: 2.8932\n",
            "Epoch 5 | Step 0 | Loss: 2.8356 | LR: 0.000095\n",
            "Epoch 5 | Step 50 | Loss: 2.6397 | LR: 0.000092\n",
            "Epoch 5 | Step 100 | Loss: 2.5281 | LR: 0.000090\n",
            "Epoch 5 | Step 150 | Loss: 2.6005 | LR: 0.000086\n",
            "Epoch 5 | Step 200 | Loss: 2.7262 | LR: 0.000082\n",
            "==> End of Epoch 5, Average Loss: 2.6006\n",
            "Epoch 6 | Step 0 | Loss: 2.5853 | LR: 0.000081\n",
            "Epoch 6 | Step 50 | Loss: 2.2798 | LR: 0.000077\n",
            "Epoch 6 | Step 100 | Loss: 2.4018 | LR: 0.000072\n",
            "Epoch 6 | Step 150 | Loss: 2.3194 | LR: 0.000068\n",
            "Epoch 6 | Step 200 | Loss: 2.3307 | LR: 0.000063\n",
            "==> End of Epoch 6, Average Loss: 2.4072\n",
            "Epoch 7 | Step 0 | Loss: 2.3268 | LR: 0.000061\n",
            "Epoch 7 | Step 50 | Loss: 2.3729 | LR: 0.000056\n",
            "Epoch 7 | Step 100 | Loss: 2.2993 | LR: 0.000051\n",
            "Epoch 7 | Step 150 | Loss: 2.2940 | LR: 0.000046\n",
            "Epoch 7 | Step 200 | Loss: 2.2284 | LR: 0.000040\n",
            "==> End of Epoch 7, Average Loss: 2.2689\n",
            "Epoch 8 | Step 0 | Loss: 2.1649 | LR: 0.000039\n",
            "Epoch 8 | Step 50 | Loss: 2.2265 | LR: 0.000034\n",
            "Epoch 8 | Step 100 | Loss: 2.3911 | LR: 0.000029\n",
            "Epoch 8 | Step 150 | Loss: 2.2178 | LR: 0.000024\n",
            "Epoch 8 | Step 200 | Loss: 2.1122 | LR: 0.000020\n",
            "==> End of Epoch 8, Average Loss: 2.1681\n",
            "Epoch 9 | Step 0 | Loss: 2.0742 | LR: 0.000019\n",
            "Epoch 9 | Step 50 | Loss: 2.1803 | LR: 0.000015\n",
            "Epoch 9 | Step 100 | Loss: 1.9926 | LR: 0.000011\n",
            "Epoch 9 | Step 150 | Loss: 1.9682 | LR: 0.000008\n",
            "Epoch 9 | Step 200 | Loss: 1.9886 | LR: 0.000006\n",
            "==> End of Epoch 9, Average Loss: 2.1032\n",
            "Epoch 10 | Step 0 | Loss: 2.0546 | LR: 0.000005\n",
            "Epoch 10 | Step 50 | Loss: 2.2342 | LR: 0.000003\n",
            "Epoch 10 | Step 100 | Loss: 2.1465 | LR: 0.000001\n",
            "Epoch 10 | Step 150 | Loss: 2.1358 | LR: 0.000000\n",
            "Epoch 10 | Step 200 | Loss: 2.2495 | LR: 0.000000\n",
            "==> End of Epoch 10, Average Loss: 2.0719\n",
            "Model saved to decoder_model.pth\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math\n",
        "import os\n",
        "import csv\n",
        "from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders, processors\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 1. Hyperparameters & Configuration\n",
        "# ==========================================\n",
        "class Config:\n",
        "    # Model Architecture\n",
        "    d_model = 512           # Embedding dimension\n",
        "    n_head = 8              # Number of attention heads (changed from 6 to 8)\n",
        "    n_layer = 16             # Number of decoder layers\n",
        "    vocab_size = 5000       # Size of BPE vocabulary\n",
        "    dropout = 0.1\n",
        "\n",
        "    # Training\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.0001\n",
        "    epochs = 10\n",
        "    train_test_split_ratio = 0.8 # New: Ratio for training set\n",
        "\n",
        "    # File Paths\n",
        "    # TODO: UPDATE THIS PATH to your actual dataset file (.csv or .txt)\n",
        "    dataset_path = '/content/processed_recipes.txt'\n",
        "\n",
        "    # TODO: IF USING CSV, UPDATE THIS to the specific column header containing the text\n",
        "    csv_column = 'text_column_name'\n",
        "\n",
        "    tokenizer_path = 'bpe_tokenizer.json'\n",
        "    model_save_path = 'decoder_model.pth'\n",
        "\n",
        "    # Hardware\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==========================================\n",
        "# 2. Tokenizer Setup (BPE)\n",
        "# ==========================================\n",
        "def get_training_corpus(dataset_path, column_name):\n",
        "    \"\"\"Yields text from the file (CSV or TXT) line by line.\"\"\"\n",
        "    is_csv = dataset_path.lower().endswith('.csv')\n",
        "\n",
        "    with open(dataset_path, 'r', encoding='utf-8') as f:\n",
        "        if is_csv:\n",
        "            reader = csv.DictReader(f)\n",
        "            for row in reader:\n",
        "                if row[column_name]:\n",
        "                    yield row[column_name]\n",
        "        else:\n",
        "            # Assume plain text file, one sequence per line\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    yield line.strip()\n",
        "\n",
        "def train_or_load_tokenizer(config):\n",
        "    \"\"\"\n",
        "    Trains a BPE tokenizer on the dataset if it doesn't exist,\n",
        "    otherwise loads the existing one.\n",
        "    \"\"\"\n",
        "    if os.path.exists(config.tokenizer_path):\n",
        "        print(f\"Loading existing tokenizer from {config.tokenizer_path}...\")\n",
        "        tokenizer = Tokenizer.from_file(config.tokenizer_path)\n",
        "    else:\n",
        "        print(f\"Training new BPE tokenizer on {config.dataset_path}...\")\n",
        "        # Initialize BPE tokenizer\n",
        "        tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "        tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
        "\n",
        "        # Trainer spec\n",
        "        trainer = trainers.BpeTrainer(\n",
        "            vocab_size=config.vocab_size,\n",
        "            special_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\", \"[EOS]\"]\n",
        "        )\n",
        "\n",
        "        # Train on the CSV iterator\n",
        "        tokenizer.train_from_iterator(\n",
        "            get_training_corpus(config.dataset_path, config.csv_column),\n",
        "            trainer=trainer\n",
        "        )\n",
        "\n",
        "        # Post-processing\n",
        "        tokenizer.save(config.tokenizer_path)\n",
        "        print(\"Tokenizer trained and saved.\")\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "# ==========================================\n",
        "# 3. Dataset Class\n",
        "# ==========================================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, file_path, tokenizer, block_size, column_name):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load data based on file type\n",
        "        self.data = []\n",
        "        is_csv = file_path.lower().endswith('.csv')\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            if is_csv:\n",
        "                reader = csv.DictReader(f)\n",
        "                self.data = [row[column_name] for row in reader if row[column_name]]\n",
        "            else:\n",
        "                self.data = [line.strip() for line in f.readlines() if line.strip()]\n",
        "\n",
        "        self.block_size = block_size\n",
        "\n",
        "        # We will handle padding manually in __getitem__ to ensure [BOS] and [EOS] placement\n",
        "        # self.tokenizer.enable_padding(length=block_size, pad_id=0, pad_token=\"[PAD]\")\n",
        "        # self.tokenizer.enable_truncation(max_length=block_size)\n",
        "\n",
        "        # Cache special token IDs\n",
        "        self.bos_id = self.tokenizer.token_to_id(\"[BOS]\")\n",
        "        self.eos_id = self.tokenizer.token_to_id(\"[EOS]\")\n",
        "        self.pad_id = self.tokenizer.token_to_id(\"[PAD]\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "\n",
        "        # Encode (without padding/truncation initially)\n",
        "        encoded = self.tokenizer.encode(text)\n",
        "        ids = encoded.ids\n",
        "\n",
        "        # Add BOS and EOS\n",
        "        # Ensure we have valid IDs for BOS/EOS\n",
        "        bos = [self.bos_id] if self.bos_id is not None else []\n",
        "        eos = [self.eos_id] if self.eos_id is not None else []\n",
        "\n",
        "        ids = bos + ids + eos\n",
        "\n",
        "        # Truncate if necessary (keep room for BOS/EOS)\n",
        "        if len(ids) > self.block_size:\n",
        "            ids = ids[:self.block_size]\n",
        "            # Ensure EOS is still at the end if we truncated it (optional, but good practice)\n",
        "            if self.eos_id is not None:\n",
        "                ids[-1] = self.eos_id\n",
        "\n",
        "        # Pad\n",
        "        padding_len = self.block_size - len(ids)\n",
        "        if padding_len > 0:\n",
        "            ids = ids + [self.pad_id] * padding_len\n",
        "\n",
        "        ids_tensor = torch.tensor(ids, dtype=torch.long)\n",
        "\n",
        "        return ids_tensor\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model Architecture (Decoder Only)\n",
        "# ==========================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
        "        self.n_head = config.n_head\n",
        "        self.d_model = config.d_model\n",
        "\n",
        "        # Register buffer for the causal mask (lower triangular matrix)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048))\n",
        "                                     .view(1, 1, 2048, 2048))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch, Time(seq_len), Channels(dim)\n",
        "\n",
        "        # Calculate query, key, values\n",
        "        q, k, v  = self.c_attn(x).split(self.d_model, dim=2)\n",
        "\n",
        "        # Transpose for multi-head attention: (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # Causal Attention (Masked)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "\n",
        "        # Re-assemble all head outputs side by side\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.sa = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm architecture\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(2048, config.d_model) # Max pos 2048\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Create position indices [0, 1, ..., T-1]\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "\n",
        "        # Forward pass\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Flatten for CrossEntropyLoss\n",
        "            # Logits: (B*T, VocabSize)\n",
        "            # Targets: (B*T)\n",
        "            # FIX: Ignore padding index (0)\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=0)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# ==========================================\n",
        "# 5. Training Routine\n",
        "# ==========================================\n",
        "def main():\n",
        "    # A. Setup\n",
        "    config = Config()\n",
        "    print(f\"Using device: {config.device}\")\n",
        "\n",
        "    # Check for data\n",
        "    if not os.path.exists(config.dataset_path):\n",
        "        print(f\"Error: Dataset file '{config.dataset_path}' not found.\")\n",
        "        print(\"Please update Config.dataset_path in the script to point to your actual data file.\")\n",
        "        return\n",
        "\n",
        "    # B. Tokenizer\n",
        "    tokenizer = train_or_load_tokenizer(config)\n",
        "    config.vocab_size = tokenizer.get_vocab_size()\n",
        "    print(f\"Vocabulary Size: {config.vocab_size}\")\n",
        "\n",
        "    # C. Prepare Data\n",
        "    # Determine block size from first line of CSV or TXT\n",
        "    is_csv = config.dataset_path.lower().endswith('.csv')\n",
        "    first_text = \"\"\n",
        "\n",
        "    with open(config.dataset_path, 'r', encoding='utf-8') as f:\n",
        "        if is_csv:\n",
        "            reader = csv.DictReader(f)\n",
        "            try:\n",
        "                first_row = next(reader)\n",
        "                if config.csv_column not in first_row:\n",
        "                    print(f\"Error: Column '{config.csv_column}' not found in CSV.\")\n",
        "                    print(f\"Found headers: {list(first_row.keys())}\")\n",
        "                    print(\"Please update Config.csv_column to match one of these headers.\")\n",
        "                    return\n",
        "                first_text = first_row[config.csv_column]\n",
        "            except StopIteration:\n",
        "                print(f\"Error: The CSV file '{config.dataset_path}' appears to be empty.\")\n",
        "                return\n",
        "        else:\n",
        "            for line in f:\n",
        "                if line.strip():\n",
        "                    first_text = line.strip()\n",
        "                    break\n",
        "            if not first_text:\n",
        "                print(\"Error: Dataset file appears empty.\")\n",
        "                return\n",
        "\n",
        "    # Simple heuristic for max len (add buffer for BOS/EOS)\n",
        "    dummy_enc = tokenizer.encode(first_text)\n",
        "    detected_len = len(dummy_enc.ids) + 2\n",
        "    print(f\"Detected sequence length from first row (with BOS/EOS): {detected_len}\")\n",
        "\n",
        "    block_size = detected_len\n",
        "    full_dataset = TextDataset(config.dataset_path, tokenizer, block_size, config.csv_column)\n",
        "\n",
        "    # Split dataset into training and testing sets\n",
        "    train_size = int(config.train_test_split_ratio * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    print(f\"Dataset split: Training samples = {len(train_dataset)}, Testing samples = {len(test_dataset)}\")\n",
        "\n",
        "    # Create DataLoaders for both sets\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False) # Shuffle False for consistent testing\n",
        "\n",
        "    # D. Model Initialization\n",
        "    model = TransformerDecoder(config).to(config.device)\n",
        "    print(\"Model initialized.\")\n",
        "\n",
        "    # E. Optimizer & Scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
        "\n",
        "    # Scheduler: OneCycleLR\n",
        "    total_steps = len(train_dataloader) * config.epochs # Use train_dataloader for total steps\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=config.learning_rate,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=0.3\n",
        "    )\n",
        "\n",
        "    # F. Training Loop\n",
        "    model.train()\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch in range(config.epochs):\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader): # Use train_dataloader\n",
        "            # Move to device\n",
        "            batch = batch.to(config.device)\n",
        "\n",
        "            # Autoregressive setup:\n",
        "            # Input: [BOS, A, B, C, EOS]\n",
        "            # Target: [A, B, C, EOS, PAD]\n",
        "\n",
        "            inputs = batch[:, :-1]\n",
        "            targets = batch[:, 1:]\n",
        "\n",
        "            # Forward\n",
        "            optimizer.zero_grad()\n",
        "            logits, loss = model(inputs, targets=targets)\n",
        "\n",
        "            # Backward\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if step % 50 == 0:\n",
        "                print(f\"Epoch {epoch+1} | Step {step} | Loss: {loss.item():.4f} | LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "        avg_loss = total_loss / len(train_dataloader) # Use train_dataloader for average loss\n",
        "        print(f\"==> End of Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # G. Save\n",
        "    torch.save(model.state_dict(), config.model_save_path)\n",
        "    print(f\"Model saved to {config.model_save_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "829aefff",
        "outputId": "2c3cae7d-f072-4aee-d58e-9a9f4d0973ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading tokenizer from bpe_tokenizer.json...\n",
            "Tokenizer loaded. Vocabulary Size: 5000\n",
            "Loading model from decoder_model.pth...\n",
            "Model loaded successfully.\n",
            "\n",
            "--- Recipe Generation Mode ---\n",
            "Enter a prompt to start generating a recipe. Type 'exit' to quit.\n",
            "\n",
            "Your prompt: actually, The recipe for Sweet Paniyaram (Bellam Ponganalu) is a delightful dish. For this recipe\n",
            "\n",
            "Generating recipe based on your prompt...\n",
            "\n",
            "--- Generated Recipe ---\n",
            "actually , The recipe for Sweet Paniyaram ( Bell am P ong an alu ) is a delightful dish . For this recipe , you will need the following ingredients : cup urad dal ( split , fine chopped ) to 4 to teaspoon mustard seeds teaspoon cumin seeds teaspoon urad dal ( optional ) teaspoon ginger paste ( curry leaves , adjust to taste ) 1 g reen chillies ( skip for lid . The full instructions are as follows : Add rice to a bowl with salt and green chili . In a grinder jar , add water . When the mixture comes to a thick , mash . Add ginger garlic paste and fry for few mins or till the aroma . Stir in the rava and red chilies . Add chilli powder and mix to the water and let the rice cook . Drain the moisture completely and add it . Mix well . Add 1 cup s water and blend to a smooth batter . I used for about 1 tbsp more . The batter can also add more water . If desired consistency , add more water . Cover and cook for 6\n",
            "------------------------\n",
            "\n",
            "Your prompt: Methi Chaman (Restaurant Style)\n",
            "\n",
            "Generating recipe based on your prompt...\n",
            "\n",
            "--- Generated Recipe ---\n",
            "Methi Ch aman ( Restaurant Style ) Recipes ? ' Chettinad Masala Kuzhambu Recipe - Mutton Curry ' is a Non Vegeterian favorite . You can make it in about 10 M + 30 M . The key ingredients are , Onion , Sunflower Oil , Garlic , Cumin seeds ( Jeera ), Dry Red Chillies , Ginger , Coriander Powder ( Dhania ), Cumin seeds ( Jeera ), Poppy seeds , Ginger , Cumin seeds ( Jeera ), Black pepper powder , Cinnamon Stick ( Dalchini ), Cinnamon Stick ( Dalchini ), Salt . Here â€™ s how you bring it : To begin making the M ani Bh u Kebab Recipe , firstly grind all the ingredients together in a mixer grinder using a blender or make a coarse paste using a little water ., Heat a kadai with ghee , add cumin seeds , fenugreek seeds , cumin seeds , asafoetida and cinnamon and allow it to crackle for 10 seconds . Add the ginger , garlic and saute well until the onions are translucent ., Once they are soft , add fennel seeds , ginger garlic paste , red chilli powder , garam masala and\n",
            "------------------------\n",
            "\n",
            "Your prompt: I mean, The recipe for Namak Para Recipe (Namak Pare)\n",
            "\n",
            "Generating recipe based on your prompt...\n",
            "\n",
            "--- Generated Recipe ---\n",
            "I mean , The recipe for Namak Par a Recipe ( Namak P are ) is a delightful dish . For this recipe , you will need the following ingredients : cup s milk ( or 2 hours ) 1 cup sugar ( or ghee ) cup sugar ( or water ) teaspoon baking soda 2 tablespoon s milk ) 1 tablespoon ghee teaspoon sugar ( fine chopped cup water ( or 2 g reen chilies ) 1 to 1 cup grated ginger garlic paste ( or butter ) 1 cup s ( or sliced or ghee ) 1 cup grated 1 cup s chopped ( or milk ) ( use coconut milk or oil to use teaspoon cardamom or grated or more as needed ) teaspoon salt ( or cup ghee or ghee ) 2 tablespoon s jaggery ( or more to teaspoon lemon zest ) teaspoon sugar ( or cup milk ) teaspoon cardamom powder ( optional ). The full instructions are as follows : Rinse the rice and soak for at least 15 to 18 hours . Drain the water and pour very little water to a colander . The full instructions are to be the\n",
            "------------------------\n",
            "\n",
            "Your prompt: exit\n",
            "Exiting recipe generation. Goodbye!\n"
          ]
        }
      ],
      "source": [
        "# Step:2 Model Inferencing\n",
        "\n",
        "# ==========================================\n",
        "# 1. Configuration and Setup\n",
        "# ==========================================\n",
        "import torch\n",
        "import torch.nn as nn # Added for nn.Module and nn.Linear\n",
        "import torch.nn.functional as F\n",
        "from tokenizers import Tokenizer\n",
        "import os\n",
        "import math # Added for math.sqrt\n",
        "\n",
        "# Re-defining Config class and model architecture from the training cell\n",
        "# to ensure they are available in this cell's scope.\n",
        "\n",
        "# ==========================================\n",
        "# 1. Hyperparameters & Configuration (Copied from training cell)\n",
        "# ==========================================\n",
        "class Config:\n",
        "    # Model Architecture\n",
        "    d_model = 512           # Embedding dimension\n",
        "    n_head = 8              # Number of attention heads (changed from 6 to 8)\n",
        "    n_layer = 16             # Number of decoder layers\n",
        "    vocab_size = 5000       # Size of BPE vocabulary\n",
        "    dropout = 0.1\n",
        "\n",
        "    # Training (not all needed for inference, but keeping for consistency)\n",
        "    batch_size = 32\n",
        "    learning_rate = 0.0001\n",
        "    epochs = 10\n",
        "    train_test_split_ratio = 0.8\n",
        "\n",
        "    # File Paths\n",
        "    dataset_path = '/content/processed_recipes.txt'\n",
        "    csv_column = 'text_column_name'\n",
        "    tokenizer_path = 'bpe_tokenizer.json'\n",
        "    model_save_path = 'decoder_model.pth'\n",
        "\n",
        "    # Hardware\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# ==========================================\n",
        "# 4. Model Architecture (Decoder Only) (Copied from training cell)\n",
        "# ==========================================\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.d_model % config.n_head == 0\n",
        "        self.c_attn = nn.Linear(config.d_model, 3 * config.d_model)\n",
        "        self.c_proj = nn.Linear(config.d_model, config.d_model)\n",
        "        self.n_head = config.n_head\n",
        "        self.d_model = config.d_model\n",
        "\n",
        "        # Register buffer for the causal mask (lower triangular matrix)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(2048, 2048))\n",
        "                                     .view(1, 1, 2048, 2048))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # Batch, Time(seq_len), Channels(dim)\n",
        "\n",
        "        # Calculate query, key, values\n",
        "        q, k, v  = self.c_attn(x).split(self.d_model, dim=2)\n",
        "\n",
        "        # Transpose for multi-head attention: (B, nh, T, hs)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        # Causal Attention (Masked)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        y = att @ v\n",
        "\n",
        "        # Re-assemble all head outputs side by side\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        return self.c_proj(y)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(config.d_model, 4 * config.d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * config.d_model, config.d_model),\n",
        "            nn.Dropout(config.dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln1 = nn.LayerNorm(config.d_model)\n",
        "        self.sa = CausalSelfAttention(config)\n",
        "        self.ln2 = nn.LayerNorm(config.d_model)\n",
        "        self.ffwd = FeedForward(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-norm architecture\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.position_embedding = nn.Embedding(2048, config.d_model) # Max pos 2048\n",
        "\n",
        "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Create position indices [0, 1, ..., T-1]\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)\n",
        "\n",
        "        # Forward pass\n",
        "        tok_emb = self.token_embedding(idx)\n",
        "        pos_emb = self.position_embedding(pos)\n",
        "        x = tok_emb + pos_emb\n",
        "\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            # Flatten for CrossEntropyLoss\n",
        "            # Logits: (B*T, VocabSize)\n",
        "            # Targets: (B*T)\n",
        "            # FIX: Ignore padding index (0)\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1), ignore_index=0)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "# Create an instance of the Config class to access paths and device settings\n",
        "config = Config()\n",
        "\n",
        "# ==========================================\n",
        "# 2. Load Tokenizer\n",
        "# ==========================================\n",
        "print(f\"Loading tokenizer from {config.tokenizer_path}...\")\n",
        "if not os.path.exists(config.tokenizer_path):\n",
        "    print(f\"Error: Tokenizer file '{config.tokenizer_path}' not found.\")\n",
        "    print(\"Please ensure the tokenizer is trained and saved in the previous step.\")\n",
        "    exit()\n",
        "tokenizer = Tokenizer.from_file(config.tokenizer_path)\n",
        "config.vocab_size = tokenizer.get_vocab_size() # Update vocab_size from loaded tokenizer\n",
        "print(f\"Tokenizer loaded. Vocabulary Size: {config.vocab_size}\")\n",
        "\n",
        "# Cache special token IDs for easier use\n",
        "bos_id = tokenizer.token_to_id(\"[BOS]\")\n",
        "eos_id = tokenizer.token_to_id(\"[EOS]\")\n",
        "pad_id = tokenizer.token_to_id(\"[PAD]\")\n",
        "\n",
        "# ==========================================\n",
        "# 3. Load Model\n",
        "# ==========================================\n",
        "print(f\"Loading model from {config.model_save_path}...\")\n",
        "if not os.path.exists(config.model_save_path):\n",
        "    print(f\"Error: Model file '{config.model_save_path}' not found.\")\n",
        "    print(\"Please ensure the model is trained and saved in the previous step.\")\n",
        "    exit()\n",
        "\n",
        "# Instantiate the model (assuming TransformerDecoder class is defined globally)\n",
        "model = TransformerDecoder(config).to(config.device)\n",
        "model.load_state_dict(torch.load(config.model_save_path, map_location=config.device))\n",
        "model.eval() # Set model to evaluation mode for inference\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. Text Generation Function\n",
        "# ==========================================\n",
        "def generate_text(model, tokenizer, prompt_text, max_length=200, temperature=0.7, top_k=40, device='cpu'):\n",
        "    \"\"\"\n",
        "    Generates text from a given prompt using the trained model.\n",
        "\n",
        "    Args:\n",
        "        model: The trained TransformerDecoder model.\n",
        "        tokenizer: The BPE tokenizer.\n",
        "        prompt_text (str): The initial text prompt.\n",
        "        max_length (int): Maximum length of the generated sequence.\n",
        "        temperature (float): Controls randomness in sampling. Lower values make output more deterministic.\n",
        "        top_k (int): If not None, sample from the top_k most probable tokens.\n",
        "        device (str): The device to run inference on ('cpu' or 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        str: The generated text.\n",
        "    \"\"\"\n",
        "    # Encode the prompt text\n",
        "    encoded_prompt = tokenizer.encode(prompt_text)\n",
        "    input_ids = encoded_prompt.ids\n",
        "\n",
        "    # Prepend BOS token if it's available and not already at the start\n",
        "    if bos_id is not None and (not input_ids or input_ids[0] != bos_id):\n",
        "        input_ids = [bos_id] + input_ids\n",
        "\n",
        "    # Convert to tensor and add batch dimension (batch_size=1)\n",
        "    input_ids_tensor = torch.tensor(input_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # List to store all generated token IDs, starting with the prompt's IDs\n",
        "    generated_ids = list(input_ids)\n",
        "\n",
        "    with torch.no_grad(): # Disable gradient calculations for inference\n",
        "        for _ in range(max_length - len(input_ids)): # Generate tokens up to max_length\n",
        "            # Get predictions (logits) from the model\n",
        "            logits, _ = model(input_ids_tensor)\n",
        "            # Focus on the logits for the last predicted token\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            # Apply top-k sampling to logits\n",
        "            if top_k is not None:\n",
        "                # Get top_k values and their indices\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                # Set logits of tokens outside the top_k to a very low value (effectively zero probability)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            # Convert logits to probabilities and sample the next token\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "\n",
        "            # Append the newly generated token to the input tensor for the next step\n",
        "            input_ids_tensor = torch.cat((input_ids_tensor, next_token_id.unsqueeze(0)), dim=1)\n",
        "            generated_ids.append(next_token_id.item()) # Add to our tracking list\n",
        "\n",
        "            # Stop generation if EOS token is produced\n",
        "            if next_token_id.item() == eos_id:\n",
        "                break\n",
        "\n",
        "    # Decode the complete sequence of token IDs back into text\n",
        "    # Remove the BOS token if it was prepended by this function and is not part of desired output\n",
        "    if bos_id is not None and generated_ids and generated_ids[0] == bos_id:\n",
        "        generated_ids_for_decode = generated_ids[1:]\n",
        "    else:\n",
        "        generated_ids_for_decode = generated_ids\n",
        "\n",
        "    return tokenizer.decode(generated_ids_for_decode, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# ==========================================\n",
        "# 5. Interactive User Input and Generation Loop\n",
        "# ==========================================\n",
        "print(\"\\n--- Recipe Generation Mode ---\")\n",
        "print(\"Enter a prompt to start generating a recipe. Type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_prompt = input(\"Your prompt: \")\n",
        "    if user_prompt.lower() == 'exit':\n",
        "        break\n",
        "\n",
        "    if not user_prompt.strip():\n",
        "        print(\"Please enter a non-empty prompt.\")\n",
        "        continue\n",
        "\n",
        "    print(\"\\nGenerating recipe based on your prompt...\")\n",
        "    generated_recipe = generate_text(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        prompt_text=user_prompt,\n",
        "        max_length=200, # Max tokens for the generated output\n",
        "        temperature=0.8, # Adjust for creativity (higher = more creative, lower = more focused)\n",
        "        top_k=50,        # Adjust to control diversity (higher = more diverse token choices)\n",
        "        device=config.device\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Generated Recipe ---\")\n",
        "    print(generated_recipe)\n",
        "    print(\"------------------------\\n\")\n",
        "\n",
        "print(\"Exiting recipe generation. Goodbye!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HucaJmFVQY4j"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO2kBW7mx/CRtNyNXt3XIQx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}